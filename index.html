<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<title>Hyuhng Joon Kim</title>
	<meta name="description" content="Hyuhng Joon Kim's homepage">

	<link href="./css/github-light.css" rel="stylesheet">
	<link href="./css/style.css" rel="stylesheet">
	<script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>
</head>
<body>
	<!-- <div class="container">
		<img src="./assets/blue.jpeg">
	</div> -->
  	<div class="container">
    <header>
		<h1>Hyuhng Joon Kim</h1>
		<img src="./assets/heyjoonkim.PNG" style="width:190px;"><br/>
		<p>
			<b>heyjoonkim (at) gmail.com<br/></b>
			Ph.D. student<br/>
			<a href="https://gsai.snu.ac.kr/">Graduate School of Artificial Intelligence</a>, SNU<br/>
			<a href="https://scholar.google.com/citations?user=NHFFB4gAAAAJ&hl=en">Google Scholar</a><br/>
			<a href="https://www.semanticscholar.org/author/Hyuhng-Joon-Kim/2166352356">Semantic Scholar</a><br/>
			<a href="https://github.com/heyjoonkim">Github</a><br/><br/>
			<a href="https://hits.sh/heyjoonkim.github.io/"><img alt="Hits" src="https://hits.sh/heyjoonkim.github.io.svg?color=9f9f9f"/></a></br>
<!-- 			<a href="https://clustrmaps.com/site/1c5sb"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=Ah6b1jOKewtU4JyFfjLFkBi0ix9IjYZm22BveDe4wHo&cl=ffffff" /></a> -->
<!-- 			<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=a&t=n&d=Ah6b1jOKewtU4JyFfjLFkBi0ix9IjYZm22BveDe4wHo&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
		</p>
    </header>

    <section class="content">
		<h3>About</h3>
			<p>
				Hello, I am a Ph.D. candidate in the Graduate School of Artificial Intelligence at Seoul National University.
				I am currently a member of <a href="http://ids.snu.ac.kr/site/">IDS lab</a>. 
				My research focuses on advancing Natural Language Processing (NLP) by developing <b>robust models that recognize their own limits</b>.
				By equipping LLMs with <b>abstention capabilities</b>, I aim to build systems you can truly rely on. 
    				</br>
				If my research interests overlap or you’d like to learn more about my previous work, please don’t hesitate to connect with me. &#128516;
				I would be delighted to explore potential collaborations! &#x1F389;
			</p>

		<h3>Education</h3>
			<ul>
				<li>2020.09 ~ present : MS. and Ph.D. (integrated) in <a href="https://gsai.snu.ac.kr/">Graduate School of Artificial Intelligence</a>, Seoul National University</li>
				<ul>
					<li>Advisor : Prof. <a href="http://ids.snu.ac.kr/site/members/M_Sang-goo_Lee.html">Sang-goo Lee</a></li>
				</ul>
				<li>2014.03 ~ 2020.08 : B.S. in  <a href="http://cse.ssu.ac.kr/">Computer Science and Engineering</a>, Soongsil University</li>
			</ul>
		
		<h3>Work Experience</h3>
			<ul>
				<li>2020 : SWE intern in <a href="http://tomatosystem.co.kr/solution/#">Tomato System</a></li>
				<li>2019 : SWE intern in <a href="navercorp.com">Naver Corp.</a></li>
			</ul>

      	<h3>Recent Publications (<a href="https://scholar.google.com/citations?user=NHFFB4gAAAAJ&hl=en">Google Scholar</a>)</h3>
	    		<ol class="sparse-list">
				<li> <b><a href="https://arxiv.org/abs/2412.12527">When to Speak, When to Abstain: Contrastive Decoding with Abstention</a></b><br/>
					<b>Hyuhng Joon Kim</b>, Youna Kim, Sang-goo Lee, Taeuk Kim<br/>
					<i>under-review</i>
				</li>		
				
				<li> <b><a href="https://arxiv.org/abs/2502.13648">Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs</a></b><br/>
					Youna Kim, Minjoon Choi, Sungmin Cho, <b>Hyuhng Joon Kim</b>, Sang-goo Lee, Taeuk Kim<br/>
					<i>under-review</i>
				</li>		
				
				<li> <b><a href="https://aclanthology.org/2024.emnlp-main.119/">Aligning Language Models to Explicitly Handle Ambiguity</a></b><br/>
					<b>Hyuhng Joon Kim</b>, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang Min Yoo, Sang-goo Lee, Taeuk Kim<br/>
					<i>The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024)</i>
				</li>		

				<li> <b><a href="https://aclanthology.org/2024.findings-emnlp.136/">Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts</a></b><br/>
					Youna Kim, <b>Hyuhng Joon Kim</b>, Cheonbok Park, Choonghyun Park, Hyunsoo Cho, Junyeob Kim, Kang Min Yoo, Sang-goo Lee, Taeuk Kim<br/>
					<i>Findings of the Association for Computational Linguistics: EMNLP 2024 (Findings of EMNLP 2024)</i>
				</li>		
				
				<li> <b><a href="https://aclanthology.org/2023.findings-emnlp.392/">Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP</a></b><br/>
					<b>Hyuhng Joon Kim</b>, Hyunsoo Cho, Sang-Woo Lee, Junyeob Kim, Choonghyun Park, Sang-goo Lee, Kang Min Yoo, Taeuk Kim<br/>
					<i>Findings of the Association for Computational Linguistics: EMNLP 2023 (Findings of EMNLP 2023)</i>
				</li>		

				<li> <b><a href="https://aclanthology.org/2023.starsem-1.21/">Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning</a></b><br/>
					Hyunsoo Cho, Choonghyun Park, Junyeop Kim, <b>Hyuhng Joon Kim</b>, Kang Min Yoo, Sang-goo Lee</br>
					<i>The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)</i>
				</li>		
				
				<li> <b><a href="https://arxiv.org/abs/2212.10873">Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners</a></b><br/>
					Hyunsoo Cho, <b>Hyuhng Joon Kim</b>, Jun Yeob Kim, Sang-Woo Lee, Sang-goo Lee, Kang Min Yoo, Taeuk Kim<br/>
					<i>Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI 2023)</i>
				</li>		

				<li> <b><a href="https://arxiv.org/abs/2206.08082">Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</a></b><br/>
					<b>Hyuhng Joon Kim</b>, Hyunsoo Cho, Jun Yeob Kim, Taeuk Kim, Kang Min Yoo, Sang-goo Lee<br/>
					<i>Workshop on Large-scale Pre-trained Language Models 2022 (NAACL Workshop)</i>
				</li>		

				<li> <b><a href="https://arxiv.org/abs/2205.12685">Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations</a></b><br/>
					Kang Min Yoo, Jun Yeob Kim, <b>Hyuhng Joon Kim</b>, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, Taeuk Kim<br/>
					<i>The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)</i>
				</li>		
			</ol>


		</section>
	</div>
</body>

</html>
